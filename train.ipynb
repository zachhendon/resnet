{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4ce2eeb438d334a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data.dataset import data_loader\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:38:02.424727Z",
     "start_time": "2024-04-17T05:38:00.843294Z"
    }
   },
   "id": "2ac06cbc44f780e7",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": "(352, 40, 79)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader = data_loader(\n",
    "    data_dir='./data/datasets', batch_size=128\n",
    ")\n",
    "test_loader = data_loader(\n",
    "    data_dir='./data/datasets', batch_size=128,\n",
    "    test=True\n",
    ")\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:38:29.305023Z",
     "start_time": "2024-04-17T05:38:28.278995Z"
    }
   },
   "id": "15b45454eb69c31b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        stride = 2 if downsample else 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        if downsample:\n",
    "            self.downsampleLayer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsampleLayer(x)\n",
    "        out += residual\n",
    "        out = nn.ReLU()(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:08:14.535585Z",
     "start_time": "2024-04-15T19:08:14.531892Z"
    }
   },
   "id": "2358822fa552f079",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_layer(in_channels, out_channels, num_blocks):\n",
    "    downsample = in_channels != out_channels\n",
    "    blocks = [ResidualBlock(in_channels, out_channels, downsample=downsample)]\n",
    "\n",
    "    for _ in range(num_blocks - 1):\n",
    "        blocks.append(ResidualBlock(out_channels, out_channels))\n",
    "    return nn.Sequential(*blocks)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "        self.layer2 = get_layer(16, 16, 9)\n",
    "        self.layer3 = get_layer(16, 32, 9)\n",
    "        self.layer4 = get_layer(32, 64, 9)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=8)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:51:56.420080Z",
     "start_time": "2024-04-15T19:51:56.416307Z"
    }
   },
   "id": "77a77e6133ab9c04",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    EPOCHS = epochs\n",
    "    train_samples_num = 45000\n",
    "    val_samples_num = 5000\n",
    "    train_costs, val_costs = [], []\n",
    "    \n",
    "    #Training phase.    \n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        train_running_loss = 0\n",
    "        correct_train = 0\n",
    "        \n",
    "        model.train().cuda()\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            \"\"\" for every mini-batch during the training phase, we typically want to explicitly set the gradients \n",
    "            to zero before starting to do backpropragation \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Start the forward pass\n",
    "            prediction = model(inputs)\n",
    "                        \n",
    "            loss = criterion(prediction, labels)\n",
    "          \n",
    "            # do backpropagation and update weights with step()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "            \n",
    "            # print('outputs on which to apply torch.max ', prediction)\n",
    "            # find the maximum along the rows, use dim=1 to torch.max()\n",
    "            _, predicted_outputs = torch.max(prediction.data, 1)\n",
    "            \n",
    "            # Update the running corrects \n",
    "            correct_train += (predicted_outputs == labels).float().sum().item()\n",
    "            \n",
    "            ''' Compute batch loss\n",
    "            multiply each average batch loss with batch-length. \n",
    "            The batch-length is inputs.size(0) which gives the number total images in each batch. \n",
    "            Essentially I am un-averaging the previously calculated Loss '''\n",
    "            train_running_loss += (loss.data.item() * inputs.shape[0])\n",
    "\n",
    "\n",
    "        train_epoch_loss = train_running_loss / train_samples_num\n",
    "        \n",
    "        train_costs.append(train_epoch_loss)\n",
    "        \n",
    "        train_acc =  correct_train / train_samples_num\n",
    "\n",
    "        # Now check trained weights on the validation set\n",
    "        val_running_loss = 0\n",
    "        correct_val = 0\n",
    "      \n",
    "        model.eval().cuda()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass.\n",
    "                prediction = model(inputs)\n",
    "\n",
    "                # Compute the loss.\n",
    "                loss = criterion(prediction, labels)\n",
    "\n",
    "                # Compute validation accuracy.\n",
    "                _, predicted_outputs = torch.max(prediction.data, 1)\n",
    "                correct_val += (predicted_outputs == labels).float().sum().item()\n",
    "\n",
    "            # Compute batch loss.\n",
    "            val_running_loss += (loss.data.item() * inputs.shape[0])\n",
    "\n",
    "            val_epoch_loss = val_running_loss / val_samples_num\n",
    "            val_costs.append(val_epoch_loss)\n",
    "            val_acc =  correct_val / val_samples_num\n",
    "        \n",
    "        info = \"[Epoch {}/{}]: train-loss = {:0.6f} | train-acc = {:0.3f} | val-loss = {:0.6f} | val-acc = {:0.3f}\"\n",
    "        \n",
    "        print(info.format(epoch+1, EPOCHS, train_epoch_loss, train_acc, val_epoch_loss, val_acc))\n",
    "        \n",
    "        torch.save(model.state_dict(), './content/checkpoint_gpu_{}'.format(epoch + 1)) \n",
    "                                                                \n",
    "    torch.save(model.state_dict(), './content/resnet-56_weights_gpu')  \n",
    "        \n",
    "    return train_costs, val_costs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:38:05.617539Z",
     "start_time": "2024-04-15T19:38:05.613134Z"
    }
   },
   "id": "37bfe1d359fcf588",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = ResNet(10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:52:03.116014Z",
     "start_time": "2024-04-15T19:52:03.104333Z"
    }
   },
   "id": "f0afcd66a8846c15",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10]: train-loss = 2.212842 | train-acc = 0.239 | val-loss = 0.003716 | val-acc = 0.252\n",
      "[Epoch 2/10]: train-loss = 2.153141 | train-acc = 0.301 | val-loss = 0.003555 | val-acc = 0.299\n",
      "[Epoch 3/10]: train-loss = 2.115063 | train-acc = 0.340 | val-loss = 0.003238 | val-acc = 0.354\n",
      "[Epoch 4/10]: train-loss = 2.094359 | train-acc = 0.362 | val-loss = 0.002937 | val-acc = 0.339\n",
      "[Epoch 5/10]: train-loss = 2.072569 | train-acc = 0.384 | val-loss = 0.003337 | val-acc = 0.394\n",
      "[Epoch 6/10]: train-loss = 2.027759 | train-acc = 0.429 | val-loss = 0.003370 | val-acc = 0.431\n",
      "[Epoch 7/10]: train-loss = 1.966469 | train-acc = 0.490 | val-loss = 0.003141 | val-acc = 0.493\n",
      "[Epoch 8/10]: train-loss = 1.908447 | train-acc = 0.549 | val-loss = 0.002718 | val-acc = 0.508\n",
      "[Epoch 9/10]: train-loss = 1.859220 | train-acc = 0.599 | val-loss = 0.002966 | val-acc = 0.585\n",
      "[Epoch 10/10]: train-loss = 1.825647 | train-acc = 0.633 | val-loss = 0.003153 | val-acc = 0.635\n"
     ]
    }
   ],
   "source": [
    "train_costs, val_costs = train_model(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T19:47:35.867965Z",
     "start_time": "2024-04-15T19:45:34.008616Z"
    }
   },
   "id": "f5993fe64233841f",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/175]: train-loss = 2.216754 | train-acc = 0.225 | val-loss = 0.003512 | val-acc = 0.254\n",
      "[Epoch 2/175]: train-loss = 2.164184 | train-acc = 0.289 | val-loss = 0.003693 | val-acc = 0.311\n",
      "[Epoch 3/175]: train-loss = 2.116097 | train-acc = 0.338 | val-loss = 0.002944 | val-acc = 0.353\n",
      "[Epoch 4/175]: train-loss = 2.070444 | train-acc = 0.385 | val-loss = 0.003479 | val-acc = 0.408\n",
      "[Epoch 5/175]: train-loss = 2.038387 | train-acc = 0.417 | val-loss = 0.002930 | val-acc = 0.371\n",
      "[Epoch 6/175]: train-loss = 2.010324 | train-acc = 0.445 | val-loss = 0.003282 | val-acc = 0.435\n",
      "[Epoch 7/175]: train-loss = 1.978497 | train-acc = 0.478 | val-loss = 0.003238 | val-acc = 0.508\n",
      "[Epoch 8/175]: train-loss = 1.931758 | train-acc = 0.525 | val-loss = 0.003315 | val-acc = 0.514\n",
      "[Epoch 9/175]: train-loss = 1.896959 | train-acc = 0.561 | val-loss = 0.002715 | val-acc = 0.566\n",
      "[Epoch 10/175]: train-loss = 1.862562 | train-acc = 0.596 | val-loss = 0.002929 | val-acc = 0.595\n",
      "[Epoch 11/175]: train-loss = 1.828677 | train-acc = 0.630 | val-loss = 0.002531 | val-acc = 0.623\n",
      "[Epoch 12/175]: train-loss = 1.806955 | train-acc = 0.652 | val-loss = 0.002767 | val-acc = 0.653\n",
      "[Epoch 13/175]: train-loss = 1.786520 | train-acc = 0.673 | val-loss = 0.002769 | val-acc = 0.671\n",
      "[Epoch 14/175]: train-loss = 1.766261 | train-acc = 0.693 | val-loss = 0.002821 | val-acc = 0.674\n",
      "[Epoch 15/175]: train-loss = 1.752379 | train-acc = 0.707 | val-loss = 0.002736 | val-acc = 0.691\n",
      "[Epoch 16/175]: train-loss = 1.739422 | train-acc = 0.720 | val-loss = 0.002338 | val-acc = 0.695\n",
      "[Epoch 17/175]: train-loss = 1.731676 | train-acc = 0.729 | val-loss = 0.002635 | val-acc = 0.713\n",
      "[Epoch 18/175]: train-loss = 1.719168 | train-acc = 0.741 | val-loss = 0.002737 | val-acc = 0.714\n",
      "[Epoch 19/175]: train-loss = 1.711992 | train-acc = 0.748 | val-loss = 0.002534 | val-acc = 0.711\n",
      "[Epoch 20/175]: train-loss = 1.703567 | train-acc = 0.756 | val-loss = 0.003160 | val-acc = 0.730\n",
      "[Epoch 21/175]: train-loss = 1.696683 | train-acc = 0.764 | val-loss = 0.002537 | val-acc = 0.738\n",
      "[Epoch 22/175]: train-loss = 1.686188 | train-acc = 0.774 | val-loss = 0.003548 | val-acc = 0.697\n",
      "[Epoch 23/175]: train-loss = 1.680858 | train-acc = 0.780 | val-loss = 0.003123 | val-acc = 0.721\n",
      "[Epoch 24/175]: train-loss = 1.672371 | train-acc = 0.788 | val-loss = 0.002649 | val-acc = 0.741\n",
      "[Epoch 25/175]: train-loss = 1.671205 | train-acc = 0.789 | val-loss = 0.002738 | val-acc = 0.763\n",
      "[Epoch 26/175]: train-loss = 1.662198 | train-acc = 0.797 | val-loss = 0.002429 | val-acc = 0.770\n",
      "[Epoch 27/175]: train-loss = 1.657822 | train-acc = 0.802 | val-loss = 0.002338 | val-acc = 0.758\n",
      "[Epoch 28/175]: train-loss = 1.654339 | train-acc = 0.806 | val-loss = 0.002757 | val-acc = 0.758\n",
      "[Epoch 29/175]: train-loss = 1.647341 | train-acc = 0.814 | val-loss = 0.002531 | val-acc = 0.769\n",
      "[Epoch 30/175]: train-loss = 1.643480 | train-acc = 0.817 | val-loss = 0.002681 | val-acc = 0.776\n",
      "[Epoch 31/175]: train-loss = 1.640436 | train-acc = 0.821 | val-loss = 0.002738 | val-acc = 0.765\n",
      "[Epoch 32/175]: train-loss = 1.636706 | train-acc = 0.824 | val-loss = 0.002535 | val-acc = 0.787\n",
      "[Epoch 33/175]: train-loss = 1.635027 | train-acc = 0.826 | val-loss = 0.003158 | val-acc = 0.790\n",
      "[Epoch 34/175]: train-loss = 1.627583 | train-acc = 0.833 | val-loss = 0.002577 | val-acc = 0.791\n",
      "[Epoch 35/175]: train-loss = 1.624213 | train-acc = 0.836 | val-loss = 0.002738 | val-acc = 0.784\n",
      "[Epoch 36/175]: train-loss = 1.620926 | train-acc = 0.840 | val-loss = 0.002833 | val-acc = 0.794\n",
      "[Epoch 37/175]: train-loss = 1.622550 | train-acc = 0.838 | val-loss = 0.002338 | val-acc = 0.781\n",
      "[Epoch 38/175]: train-loss = 1.616102 | train-acc = 0.844 | val-loss = 0.002506 | val-acc = 0.787\n",
      "[Epoch 39/175]: train-loss = 1.612208 | train-acc = 0.849 | val-loss = 0.002738 | val-acc = 0.797\n",
      "[Epoch 40/175]: train-loss = 1.608856 | train-acc = 0.852 | val-loss = 0.002538 | val-acc = 0.785\n",
      "[Epoch 41/175]: train-loss = 1.606410 | train-acc = 0.854 | val-loss = 0.002515 | val-acc = 0.777\n",
      "[Epoch 42/175]: train-loss = 1.608975 | train-acc = 0.852 | val-loss = 0.002932 | val-acc = 0.780\n",
      "[Epoch 43/175]: train-loss = 1.604103 | train-acc = 0.857 | val-loss = 0.002537 | val-acc = 0.793\n",
      "[Epoch 44/175]: train-loss = 1.599955 | train-acc = 0.861 | val-loss = 0.002534 | val-acc = 0.800\n",
      "[Epoch 45/175]: train-loss = 1.594461 | train-acc = 0.866 | val-loss = 0.002466 | val-acc = 0.793\n",
      "[Epoch 46/175]: train-loss = 1.594375 | train-acc = 0.866 | val-loss = 0.002340 | val-acc = 0.805\n",
      "[Epoch 47/175]: train-loss = 1.589962 | train-acc = 0.871 | val-loss = 0.002733 | val-acc = 0.807\n",
      "[Epoch 48/175]: train-loss = 1.590004 | train-acc = 0.871 | val-loss = 0.002845 | val-acc = 0.807\n",
      "[Epoch 49/175]: train-loss = 1.587514 | train-acc = 0.873 | val-loss = 0.002538 | val-acc = 0.789\n",
      "[Epoch 50/175]: train-loss = 1.588946 | train-acc = 0.872 | val-loss = 0.002734 | val-acc = 0.800\n",
      "[Epoch 51/175]: train-loss = 1.585537 | train-acc = 0.875 | val-loss = 0.003138 | val-acc = 0.807\n",
      "[Epoch 52/175]: train-loss = 1.583366 | train-acc = 0.877 | val-loss = 0.002766 | val-acc = 0.804\n",
      "[Epoch 53/175]: train-loss = 1.581033 | train-acc = 0.879 | val-loss = 0.003121 | val-acc = 0.810\n",
      "[Epoch 54/175]: train-loss = 1.578631 | train-acc = 0.882 | val-loss = 0.002338 | val-acc = 0.807\n",
      "[Epoch 55/175]: train-loss = 1.580070 | train-acc = 0.881 | val-loss = 0.002939 | val-acc = 0.788\n",
      "[Epoch 56/175]: train-loss = 1.571516 | train-acc = 0.890 | val-loss = 0.002558 | val-acc = 0.810\n",
      "[Epoch 57/175]: train-loss = 1.569869 | train-acc = 0.891 | val-loss = 0.002538 | val-acc = 0.813\n",
      "[Epoch 58/175]: train-loss = 1.571241 | train-acc = 0.889 | val-loss = 0.002738 | val-acc = 0.812\n",
      "[Epoch 59/175]: train-loss = 1.570921 | train-acc = 0.890 | val-loss = 0.002338 | val-acc = 0.803\n",
      "[Epoch 60/175]: train-loss = 1.567522 | train-acc = 0.893 | val-loss = 0.002340 | val-acc = 0.814\n",
      "[Epoch 61/175]: train-loss = 1.564560 | train-acc = 0.896 | val-loss = 0.002536 | val-acc = 0.797\n",
      "[Epoch 62/175]: train-loss = 1.565819 | train-acc = 0.895 | val-loss = 0.002493 | val-acc = 0.819\n",
      "[Epoch 63/175]: train-loss = 1.566862 | train-acc = 0.894 | val-loss = 0.002538 | val-acc = 0.820\n",
      "[Epoch 64/175]: train-loss = 1.561536 | train-acc = 0.899 | val-loss = 0.002549 | val-acc = 0.804\n",
      "[Epoch 65/175]: train-loss = 1.562035 | train-acc = 0.899 | val-loss = 0.002599 | val-acc = 0.812\n",
      "[Epoch 66/175]: train-loss = 1.559143 | train-acc = 0.902 | val-loss = 0.002938 | val-acc = 0.813\n",
      "[Epoch 67/175]: train-loss = 1.561695 | train-acc = 0.899 | val-loss = 0.002536 | val-acc = 0.802\n",
      "[Epoch 68/175]: train-loss = 1.555484 | train-acc = 0.905 | val-loss = 0.002738 | val-acc = 0.810\n",
      "[Epoch 69/175]: train-loss = 1.555619 | train-acc = 0.905 | val-loss = 0.002738 | val-acc = 0.822\n",
      "[Epoch 70/175]: train-loss = 1.554048 | train-acc = 0.907 | val-loss = 0.002546 | val-acc = 0.808\n",
      "[Epoch 71/175]: train-loss = 1.554978 | train-acc = 0.906 | val-loss = 0.002533 | val-acc = 0.820\n",
      "[Epoch 72/175]: train-loss = 1.549973 | train-acc = 0.911 | val-loss = 0.002738 | val-acc = 0.823\n",
      "[Epoch 73/175]: train-loss = 1.552690 | train-acc = 0.908 | val-loss = 0.002737 | val-acc = 0.817\n",
      "[Epoch 74/175]: train-loss = 1.553397 | train-acc = 0.908 | val-loss = 0.002732 | val-acc = 0.810\n",
      "[Epoch 75/175]: train-loss = 1.552149 | train-acc = 0.909 | val-loss = 0.002613 | val-acc = 0.824\n",
      "[Epoch 76/175]: train-loss = 1.546655 | train-acc = 0.915 | val-loss = 0.002738 | val-acc = 0.822\n",
      "[Epoch 77/175]: train-loss = 1.548210 | train-acc = 0.913 | val-loss = 0.002356 | val-acc = 0.825\n",
      "[Epoch 78/175]: train-loss = 1.547029 | train-acc = 0.914 | val-loss = 0.002549 | val-acc = 0.811\n",
      "[Epoch 79/175]: train-loss = 1.549327 | train-acc = 0.912 | val-loss = 0.002961 | val-acc = 0.817\n",
      "[Epoch 80/175]: train-loss = 1.546853 | train-acc = 0.914 | val-loss = 0.002945 | val-acc = 0.818\n",
      "[Epoch 81/175]: train-loss = 1.548714 | train-acc = 0.912 | val-loss = 0.002572 | val-acc = 0.821\n",
      "[Epoch 82/175]: train-loss = 1.545694 | train-acc = 0.915 | val-loss = 0.002937 | val-acc = 0.818\n",
      "[Epoch 83/175]: train-loss = 1.544513 | train-acc = 0.916 | val-loss = 0.002339 | val-acc = 0.818\n",
      "[Epoch 84/175]: train-loss = 1.546986 | train-acc = 0.914 | val-loss = 0.002338 | val-acc = 0.825\n",
      "[Epoch 85/175]: train-loss = 1.542829 | train-acc = 0.918 | val-loss = 0.002505 | val-acc = 0.825\n",
      "[Epoch 86/175]: train-loss = 1.544438 | train-acc = 0.916 | val-loss = 0.002502 | val-acc = 0.811\n",
      "[Epoch 87/175]: train-loss = 1.539142 | train-acc = 0.922 | val-loss = 0.003310 | val-acc = 0.824\n",
      "[Epoch 88/175]: train-loss = 1.537692 | train-acc = 0.924 | val-loss = 0.002738 | val-acc = 0.826\n",
      "[Epoch 89/175]: train-loss = 1.536606 | train-acc = 0.924 | val-loss = 0.002538 | val-acc = 0.821\n",
      "[Epoch 90/175]: train-loss = 1.536959 | train-acc = 0.924 | val-loss = 0.002738 | val-acc = 0.823\n",
      "[Epoch 91/175]: train-loss = 1.536874 | train-acc = 0.924 | val-loss = 0.002338 | val-acc = 0.826\n",
      "[Epoch 92/175]: train-loss = 1.539187 | train-acc = 0.922 | val-loss = 0.002572 | val-acc = 0.812\n",
      "[Epoch 93/175]: train-loss = 1.539491 | train-acc = 0.921 | val-loss = 0.002338 | val-acc = 0.822\n",
      "[Epoch 94/175]: train-loss = 1.538624 | train-acc = 0.923 | val-loss = 0.002538 | val-acc = 0.824\n",
      "[Epoch 95/175]: train-loss = 1.532286 | train-acc = 0.929 | val-loss = 0.002419 | val-acc = 0.821\n",
      "[Epoch 96/175]: train-loss = 1.534591 | train-acc = 0.926 | val-loss = 0.002338 | val-acc = 0.819\n",
      "[Epoch 97/175]: train-loss = 1.535353 | train-acc = 0.926 | val-loss = 0.002936 | val-acc = 0.822\n",
      "[Epoch 98/175]: train-loss = 1.534780 | train-acc = 0.926 | val-loss = 0.002706 | val-acc = 0.825\n",
      "[Epoch 99/175]: train-loss = 1.530932 | train-acc = 0.930 | val-loss = 0.002737 | val-acc = 0.819\n",
      "[Epoch 100/175]: train-loss = 1.531702 | train-acc = 0.929 | val-loss = 0.002538 | val-acc = 0.825\n",
      "[Epoch 101/175]: train-loss = 1.529561 | train-acc = 0.932 | val-loss = 0.002338 | val-acc = 0.826\n",
      "[Epoch 102/175]: train-loss = 1.530078 | train-acc = 0.931 | val-loss = 0.002750 | val-acc = 0.820\n",
      "[Epoch 103/175]: train-loss = 1.530654 | train-acc = 0.930 | val-loss = 0.002736 | val-acc = 0.813\n",
      "[Epoch 104/175]: train-loss = 1.533177 | train-acc = 0.928 | val-loss = 0.002538 | val-acc = 0.812\n",
      "[Epoch 105/175]: train-loss = 1.529414 | train-acc = 0.932 | val-loss = 0.002738 | val-acc = 0.824\n",
      "[Epoch 106/175]: train-loss = 1.529620 | train-acc = 0.931 | val-loss = 0.002733 | val-acc = 0.820\n",
      "[Epoch 107/175]: train-loss = 1.528537 | train-acc = 0.932 | val-loss = 0.002569 | val-acc = 0.820\n",
      "[Epoch 108/175]: train-loss = 1.528170 | train-acc = 0.933 | val-loss = 0.002738 | val-acc = 0.827\n",
      "[Epoch 109/175]: train-loss = 1.525400 | train-acc = 0.936 | val-loss = 0.002538 | val-acc = 0.829\n",
      "[Epoch 110/175]: train-loss = 1.527598 | train-acc = 0.933 | val-loss = 0.002540 | val-acc = 0.810\n",
      "[Epoch 111/175]: train-loss = 1.529225 | train-acc = 0.932 | val-loss = 0.002736 | val-acc = 0.822\n",
      "[Epoch 112/175]: train-loss = 1.529193 | train-acc = 0.932 | val-loss = 0.002338 | val-acc = 0.818\n",
      "[Epoch 113/175]: train-loss = 1.523317 | train-acc = 0.938 | val-loss = 0.002738 | val-acc = 0.827\n",
      "[Epoch 114/175]: train-loss = 1.524919 | train-acc = 0.936 | val-loss = 0.002937 | val-acc = 0.819\n",
      "[Epoch 115/175]: train-loss = 1.524890 | train-acc = 0.936 | val-loss = 0.002738 | val-acc = 0.827\n",
      "[Epoch 116/175]: train-loss = 1.523140 | train-acc = 0.938 | val-loss = 0.002338 | val-acc = 0.831\n",
      "[Epoch 117/175]: train-loss = 1.530583 | train-acc = 0.930 | val-loss = 0.002938 | val-acc = 0.812\n",
      "[Epoch 118/175]: train-loss = 1.524015 | train-acc = 0.937 | val-loss = 0.002743 | val-acc = 0.828\n",
      "[Epoch 119/175]: train-loss = 1.523948 | train-acc = 0.937 | val-loss = 0.002338 | val-acc = 0.826\n",
      "[Epoch 120/175]: train-loss = 1.524501 | train-acc = 0.937 | val-loss = 0.002338 | val-acc = 0.835\n",
      "[Epoch 121/175]: train-loss = 1.524837 | train-acc = 0.936 | val-loss = 0.002538 | val-acc = 0.821\n",
      "[Epoch 122/175]: train-loss = 1.524572 | train-acc = 0.936 | val-loss = 0.002738 | val-acc = 0.829\n",
      "[Epoch 123/175]: train-loss = 1.522768 | train-acc = 0.938 | val-loss = 0.002338 | val-acc = 0.828\n",
      "[Epoch 124/175]: train-loss = 1.519129 | train-acc = 0.942 | val-loss = 0.002540 | val-acc = 0.835\n",
      "[Epoch 125/175]: train-loss = 1.520505 | train-acc = 0.941 | val-loss = 0.002736 | val-acc = 0.831\n",
      "[Epoch 126/175]: train-loss = 1.521142 | train-acc = 0.940 | val-loss = 0.002535 | val-acc = 0.819\n",
      "[Epoch 127/175]: train-loss = 1.523346 | train-acc = 0.938 | val-loss = 0.002557 | val-acc = 0.821\n",
      "[Epoch 128/175]: train-loss = 1.518382 | train-acc = 0.943 | val-loss = 0.002535 | val-acc = 0.822\n",
      "[Epoch 129/175]: train-loss = 1.517889 | train-acc = 0.943 | val-loss = 0.002338 | val-acc = 0.822\n",
      "[Epoch 130/175]: train-loss = 1.519152 | train-acc = 0.942 | val-loss = 0.002738 | val-acc = 0.829\n",
      "[Epoch 131/175]: train-loss = 1.518595 | train-acc = 0.942 | val-loss = 0.002537 | val-acc = 0.821\n",
      "[Epoch 132/175]: train-loss = 1.521267 | train-acc = 0.940 | val-loss = 0.002738 | val-acc = 0.824\n",
      "[Epoch 133/175]: train-loss = 1.517634 | train-acc = 0.943 | val-loss = 0.003090 | val-acc = 0.815\n",
      "[Epoch 134/175]: train-loss = 1.518349 | train-acc = 0.943 | val-loss = 0.002538 | val-acc = 0.824\n",
      "[Epoch 135/175]: train-loss = 1.514459 | train-acc = 0.947 | val-loss = 0.002370 | val-acc = 0.833\n",
      "[Epoch 136/175]: train-loss = 1.515542 | train-acc = 0.946 | val-loss = 0.002937 | val-acc = 0.824\n",
      "[Epoch 137/175]: train-loss = 1.516885 | train-acc = 0.944 | val-loss = 0.002338 | val-acc = 0.833\n",
      "[Epoch 138/175]: train-loss = 1.518997 | train-acc = 0.942 | val-loss = 0.002538 | val-acc = 0.824\n",
      "[Epoch 139/175]: train-loss = 1.519056 | train-acc = 0.942 | val-loss = 0.002338 | val-acc = 0.831\n",
      "[Epoch 140/175]: train-loss = 1.517891 | train-acc = 0.943 | val-loss = 0.002946 | val-acc = 0.830\n",
      "[Epoch 141/175]: train-loss = 1.514059 | train-acc = 0.947 | val-loss = 0.002738 | val-acc = 0.822\n",
      "[Epoch 142/175]: train-loss = 1.515517 | train-acc = 0.946 | val-loss = 0.002538 | val-acc = 0.825\n",
      "[Epoch 143/175]: train-loss = 1.515940 | train-acc = 0.945 | val-loss = 0.002538 | val-acc = 0.818\n",
      "[Epoch 144/175]: train-loss = 1.513977 | train-acc = 0.947 | val-loss = 0.002530 | val-acc = 0.830\n",
      "[Epoch 145/175]: train-loss = 1.514247 | train-acc = 0.947 | val-loss = 0.002535 | val-acc = 0.826\n",
      "[Epoch 146/175]: train-loss = 1.516170 | train-acc = 0.945 | val-loss = 0.002738 | val-acc = 0.832\n",
      "[Epoch 147/175]: train-loss = 1.514706 | train-acc = 0.946 | val-loss = 0.002338 | val-acc = 0.827\n",
      "[Epoch 148/175]: train-loss = 1.513814 | train-acc = 0.947 | val-loss = 0.002339 | val-acc = 0.815\n",
      "[Epoch 149/175]: train-loss = 1.515959 | train-acc = 0.945 | val-loss = 0.002614 | val-acc = 0.825\n",
      "[Epoch 150/175]: train-loss = 1.515157 | train-acc = 0.946 | val-loss = 0.002538 | val-acc = 0.832\n",
      "[Epoch 151/175]: train-loss = 1.512016 | train-acc = 0.949 | val-loss = 0.002338 | val-acc = 0.829\n",
      "[Epoch 152/175]: train-loss = 1.512146 | train-acc = 0.949 | val-loss = 0.002538 | val-acc = 0.828\n",
      "[Epoch 153/175]: train-loss = 1.512803 | train-acc = 0.948 | val-loss = 0.002705 | val-acc = 0.834\n",
      "[Epoch 154/175]: train-loss = 1.519214 | train-acc = 0.942 | val-loss = 0.002538 | val-acc = 0.839\n",
      "[Epoch 155/175]: train-loss = 1.512796 | train-acc = 0.948 | val-loss = 0.002339 | val-acc = 0.834\n",
      "[Epoch 156/175]: train-loss = 1.512862 | train-acc = 0.948 | val-loss = 0.002738 | val-acc = 0.827\n",
      "[Epoch 157/175]: train-loss = 1.513120 | train-acc = 0.948 | val-loss = 0.002938 | val-acc = 0.831\n",
      "[Epoch 158/175]: train-loss = 1.511677 | train-acc = 0.950 | val-loss = 0.002522 | val-acc = 0.834\n",
      "[Epoch 159/175]: train-loss = 1.513390 | train-acc = 0.948 | val-loss = 0.002538 | val-acc = 0.831\n",
      "[Epoch 160/175]: train-loss = 1.514475 | train-acc = 0.946 | val-loss = 0.002938 | val-acc = 0.836\n",
      "[Epoch 161/175]: train-loss = 1.509018 | train-acc = 0.952 | val-loss = 0.002338 | val-acc = 0.829\n",
      "[Epoch 162/175]: train-loss = 1.509820 | train-acc = 0.951 | val-loss = 0.002565 | val-acc = 0.829\n",
      "[Epoch 163/175]: train-loss = 1.508062 | train-acc = 0.953 | val-loss = 0.002933 | val-acc = 0.827\n",
      "[Epoch 164/175]: train-loss = 1.511164 | train-acc = 0.950 | val-loss = 0.002568 | val-acc = 0.827\n",
      "[Epoch 165/175]: train-loss = 1.509890 | train-acc = 0.951 | val-loss = 0.002925 | val-acc = 0.830\n",
      "[Epoch 166/175]: train-loss = 1.512511 | train-acc = 0.948 | val-loss = 0.002538 | val-acc = 0.836\n",
      "[Epoch 167/175]: train-loss = 1.510630 | train-acc = 0.951 | val-loss = 0.003138 | val-acc = 0.820\n",
      "[Epoch 168/175]: train-loss = 1.512295 | train-acc = 0.949 | val-loss = 0.002947 | val-acc = 0.832\n",
      "[Epoch 169/175]: train-loss = 1.509729 | train-acc = 0.951 | val-loss = 0.003137 | val-acc = 0.818\n",
      "[Epoch 170/175]: train-loss = 1.510288 | train-acc = 0.951 | val-loss = 0.002936 | val-acc = 0.841\n",
      "[Epoch 171/175]: train-loss = 1.509425 | train-acc = 0.951 | val-loss = 0.002338 | val-acc = 0.826\n",
      "[Epoch 172/175]: train-loss = 1.514801 | train-acc = 0.946 | val-loss = 0.002538 | val-acc = 0.815\n",
      "[Epoch 173/175]: train-loss = 1.513512 | train-acc = 0.947 | val-loss = 0.002938 | val-acc = 0.832\n",
      "[Epoch 174/175]: train-loss = 1.510321 | train-acc = 0.951 | val-loss = 0.002738 | val-acc = 0.837\n",
      "[Epoch 175/175]: train-loss = 1.508578 | train-acc = 0.953 | val-loss = 0.002738 | val-acc = 0.834\n"
     ]
    }
   ],
   "source": [
    "train_costs, val_costs = train_model(175)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T20:27:46.706157Z",
     "start_time": "2024-04-15T19:52:19.415502Z"
    }
   },
   "id": "2030ce4af86b8f61",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = ResNet().cuda()\n",
    "model.load_state_dict(torch.load('models/resnet-v2_20240417_013340/checkpoint_173'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:23:47.270431Z",
     "start_time": "2024-04-17T07:23:47.093753Z"
    }
   },
   "id": "daddcd50d271382",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8159\n"
     ]
    }
   ],
   "source": [
    "test_samples_num = 10000\n",
    "correct = 0 \n",
    "\n",
    "model.eval().cuda()\n",
    "\n",
    "with  torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Make predictions.\n",
    "        prediction = model(inputs)\n",
    "\n",
    "        # Retrieve predictions indexes.\n",
    "        _, predicted_class = torch.max(prediction.data, 1)\n",
    "\n",
    "        # Compute number of correct predictions.\n",
    "        correct += (predicted_class == labels).float().sum().item()\n",
    "\n",
    "test_accuracy = correct / test_samples_num\n",
    "print('Test accuracy: {}'.format(test_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T00:32:28.642253Z",
     "start_time": "2024-04-17T00:32:25.739196Z"
    }
   },
   "id": "f4137a0dde15f5aa",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ResNet(10)\n",
    "a.load_state_dict(torch.load('./content/resnet-56_weights_gpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T00:41:02.431809Z",
     "start_time": "2024-04-17T00:41:02.332224Z"
    }
   },
   "id": "3a2d36860937bd78",
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (layer1): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (layer2): Sequential(\n    (0): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (layer3): Sequential(\n    (0): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (downsampleLayer): Sequential(\n        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (layer4): Sequential(\n    (0): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (downsampleLayer): Sequential(\n        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n  (fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=64, out_features=10, bias=True)\n    (2): Softmax(dim=1)\n  )\n)"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.eval().cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T00:41:09.292861Z",
     "start_time": "2024-04-17T00:41:09.270968Z"
    }
   },
   "id": "3346fd3a33de666",
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.3684\n"
     ]
    }
   ],
   "source": [
    "test_samples_num = 10000\n",
    "correct = 0 \n",
    "\n",
    "a.eval().cuda()\n",
    "\n",
    "with  torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Make predictions.\n",
    "        prediction = a(inputs)\n",
    "\n",
    "        # Retrieve predictions indexes.\n",
    "        _, predicted_class = torch.max(prediction.data, 1)\n",
    "\n",
    "        # Compute number of correct predictions.\n",
    "        correct += (predicted_class == labels).float().sum().item()\n",
    "\n",
    "test_accuracy = correct / test_samples_num\n",
    "print('Test accuracy: {}'.format(test_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T00:41:29.557764Z",
     "start_time": "2024-04-17T00:41:27.834421Z"
    }
   },
   "id": "a29c8061dc9cc8ae",
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be955a4806a5edc3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b1844e71b823195f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f9e2b2012aef13fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "24050a2852ff3b67"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batch loss: 1.6217\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(test_loader))\n",
    "inputs, labels = inputs.cuda(), labels.cuda()\n",
    "pred = model(inputs)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(pred, labels).item()\n",
    "print(f\"Total batch loss: {loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T04:04:17.475653Z",
     "start_time": "2024-04-17T04:04:17.296743Z"
    }
   },
   "id": "5a699993159e280c",
   "execution_count": 285
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1, 3], gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T04:04:17.835972Z",
     "start_time": "2024-04-17T04:04:17.832956Z"
    }
   },
   "id": "fcefd5f1ad8eced2",
   "execution_count": 286
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_training = sum(inputs.size(0) for inputs, _ in train_loader)\n",
    "num_val = sum(inputs.size(0) for inputs, _ in val_loader)\n",
    "\n",
    "def train_epoch(model):\n",
    "    running_loss = 0.\n",
    "    num_correct = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item() * len(inputs)\n",
    "        \n",
    "        class_outputs = torch.argmax(outputs, dim=1)\n",
    "        num_correct += (class_outputs == labels).sum()\n",
    "        \n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = running_loss / num_training\n",
    "    avg_acc = num_correct / num_training\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T04:23:39.129304Z",
     "start_time": "2024-04-17T04:23:35.828181Z"
    }
   },
   "id": "954a93f112562506",
   "execution_count": 310
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T04:19:51.256337Z",
     "start_time": "2024-04-17T04:19:51.253915Z"
    }
   },
   "id": "bbd59c6ed8e78529",
   "execution_count": 304
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "[Epoch 1/3]: train-loss = 1.5002 | train-acc = 0.961 | val-loss = 1.6199 | val-acc = 0.8398\n",
      "[Epoch 2/3]: train-loss = 1.4994 | train-acc = 0.962 | val-loss = 1.6183 | val-acc = 0.8428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[311], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[1;32m      9\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 10\u001B[0m     avg_train_loss, avg_train_acc \u001B[38;5;241m=\u001B[39m train_epoch(model)\n\u001B[1;32m     11\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     13\u001B[0m     running_val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n",
      "Cell \u001B[0;32mIn[310], line 13\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m      9\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mcuda(), labels\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 13\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     14\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, labels)\n\u001B[1;32m     15\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[86], line 31\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     29\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[1;32m     30\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n\u001B[0;32m---> 31\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer4(x)\n\u001B[1;32m     32\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavgpool(x)\n\u001B[1;32m     33\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(x)\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[6], line 30\u001B[0m, in \u001B[0;36mResidualBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     28\u001B[0m     residual \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsampleLayer(x)\n\u001B[1;32m     29\u001B[0m out \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m residual\n\u001B[0;32m---> 30\u001B[0m out \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mReLU()(out)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/activation.py:98\u001B[0m, in \u001B[0;36mReLU.__init__\u001B[0;34m(self, inplace)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inplace: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m---> 98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minplace \u001B[38;5;241m=\u001B[39m inplace\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1735\u001B[0m, in \u001B[0;36mModule.__setattr__\u001B[0;34m(self, name, value)\u001B[0m\n\u001B[1;32m   1733\u001B[0m     modules[name] \u001B[38;5;241m=\u001B[39m value\n\u001B[1;32m   1734\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1735\u001B[0m     buffers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_buffers\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   1736\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m buffers:\n\u001B[1;32m   1737\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, torch\u001B[38;5;241m.\u001B[39mTensor):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "best_val_loss = torch.inf\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f\"models/resnet_{timestamp}\")\n",
    "\n",
    "print('Starting training...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(True)\n",
    "    avg_train_loss, avg_train_acc = train_epoch(model)\n",
    "    model.eval()\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()\n",
    "            \n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = loss_fn(val_outputs, val_labels)\n",
    "            running_val_loss += val_loss * len(val_inputs)\n",
    "            \n",
    "            _, class_val_outputs = torch.max(val_outputs.data, dim=1)\n",
    "            val_correct += (class_val_outputs == val_labels).int().sum().item()\n",
    "    avg_val_loss = running_val_loss / num_val\n",
    "    avg_val_acc = val_correct / num_val\n",
    "    \n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS}]: train-loss = {avg_train_loss:.4f} | train-acc = {avg_train_acc:.3f} \"\n",
    "          f\"| val-loss = {avg_val_loss:.4f} | val-acc = {avg_val_acc}\")\n",
    "\n",
    "    writer.add_scalars('Training vs Validation Loss',\n",
    "                       {'Training': avg_train_loss, 'Validation': avg_val_loss},\n",
    "                       epoch + 1)\n",
    "    writer.add_scalars('Training vs Validation Accuracy',\n",
    "                       {'Training': avg_train_acc, 'Validation': avg_val_acc},\n",
    "                       epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model_path = f\"models/resnet_{timestamp}/resnet_{epoch}\"\n",
    "        torch.save(model.state_dict(), model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T04:24:07.209147Z",
     "start_time": "2024-04-17T04:23:39.130315Z"
    }
   },
   "id": "fa0798d223cd2831",
   "execution_count": 311
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac84280e2daa08"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Module.modules at 0x7f3760ca0930>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from model import ResNet\n",
    "model = ResNet().cuda()\n",
    "model.load_state_dict(torch.load('models/resnet-v2_20240417_013340/checkpoint_173'))\n",
    "\n",
    "model.modules()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:24:48.016642Z",
     "start_time": "2024-04-17T07:24:47.244684Z"
    }
   },
   "id": "3059c53fe7fbfc54",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'bias'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mmodules():\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[0;32m~/anaconda3/envs/resnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1687\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1688\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ResNet' object has no attribute 'bias'"
     ]
    }
   ],
   "source": [
    "for m in model.modules():\n",
    "    # print(m.bias)\n",
    "    p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T07:37:10.203888Z",
     "start_time": "2024-04-17T07:37:10.192870Z"
    }
   },
   "id": "5235e679cc4b7595",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7b077deeaad96ef1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "df715f50a2d57dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21eaedef7329688a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([17,  0,  9, 12, 16,  7, 11, 18, 13, 19, 15,  4, 10, 14,  3,  8,  2,  5,\n         1,  6])"
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T05:32:37.133894Z",
     "start_time": "2024-04-17T05:32:37.130070Z"
    }
   },
   "id": "625f7f4e203da2a6",
   "execution_count": 315
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
